# task1: CartPole DQN Trainer with MLP and CLI support

import os
import argparse
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import deque
import matplotlib.pyplot as plt
import imageio

# ==== CLI ==== 
# è¨­å®šå‘½ä»¤åˆ—ä»‹é¢ (Command-Line Interface)ï¼Œè®“ä½¿ç”¨è€…å¯ä»¥å¾å¤–éƒ¨å‚³å…¥åƒæ•¸
parser = argparse.ArgumentParser()
# æ–°å¢ '--resume' åƒæ•¸ï¼Œç”¨æ–¼æŒ‡å®šå·²å„²å­˜æ¨¡å‹çš„è·¯å¾‘ï¼Œä»¥ä¾¿æ¥çºŒè¨“ç·´
parser.add_argument('--resume', type=str, help='path to checkpoint')
parser.add_argument('--episodes', type=int, default=10000)
args = parser.parse_args()

# ==== è¶…åƒæ•¸ ====
GAMMA = 0.99             # æŠ˜æ‰£å› å­ï¼Œæœªä¾†çå‹µçš„é‡è¦æ€§
LR = 1e-3                # å­¸ç¿’ç‡ (Learning Rate)ï¼Œæ¨¡å‹æ›´æ–°çš„æ­¥ä¼å¤§å°
MEMORY_SIZE = 10000      # ç¶“é©—å›æ”¾ç·©è¡å€ (Replay Buffer) çš„æœ€å¤§å®¹é‡
BATCH_SIZE = 64          
TARGET_UPDATE = 100      # ç›®æ¨™ç¶²è·¯ (Target Network) æ›´æ–°çš„é »ç‡ï¼ˆæ¯éš”å¤šå°‘æ­¥ï¼‰
EPSILON_START = 1.0      # Epsilon-Greedy ç­–ç•¥çš„åˆå§‹æ¢ç´¢ç‡ (å®Œå…¨éš¨æ©Ÿ)
EPSILON_END = 0.01       # æœ€çµ‚æ¢ç´¢ç‡ (å¹¾ä¹ä¸éš¨æ©Ÿ)
EPSILON_DECAY = 500      # æ¢ç´¢ç‡ä¸‹é™çš„é€Ÿç‡

os.makedirs("../history_task1", exist_ok=True)
os.makedirs("../best_task1", exist_ok=True)
os.makedirs("../pic_task1", exist_ok=True)
os.makedirs("../vid_task1", exist_ok=True)

device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
print("âœ… Using device:", device)

# ==== MLP ==== 
# å®šç¾©ç¥ç¶“ç¶²è·¯çš„çµæ§‹
class MLP(nn.Module):
    def __init__(self, input_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128), nn.ReLU(),
            nn.Linear(128, 128), nn.ReLU(),
            nn.Linear(128, action_dim)    # éš±è—å±¤ 2 -> è¼¸å‡ºå±¤ (è¼¸å‡º Q-value)
        )

    def forward(self, x):
        # å®šç¾©è³‡æ–™åœ¨ç¶²è·¯ä¸­çš„å‰å‘å‚³æ’­è·¯å¾‘
        return self.net(x)

# ==== Replay Buffer ====
# ç”¨æ–¼å„²å­˜å’Œå–æ¨£æ™ºèƒ½é«”çš„ç¶“é©— (ç‹€æ…‹ã€å‹•ä½œã€çå‹µç­‰)
class ReplayBuffer:
    def __init__(self, capacity):
        # ä½¿ç”¨ collections.deque å‰µå»ºä¸€å€‹æœ‰æœ€å¤§é•·åº¦é™åˆ¶çš„ä½‡åˆ—
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        # å°‡ä¸€ç­†ç¶“é©—å­˜å…¥ç·©è¡å€
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        # å¾ç·©è¡å€ä¸­éš¨æ©ŸæŠ½å–æŒ‡å®šæ•¸é‡çš„ç¶“é©—
        batch = random.sample(self.buffer, batch_size)
        # å°‡æŠ½æ¨£å‡ºçš„ç¶“é©—è§£å£“ç¸®ä¸¦è½‰æ›æˆ PyTorch Tensors
        state, action, reward, next_state, done = zip(*batch)
        return (
            torch.FloatTensor(state).to(device),
            torch.LongTensor(action).unsqueeze(1).to(device),
            torch.FloatTensor(reward).unsqueeze(1).to(device),
            torch.FloatTensor(next_state).to(device),
            torch.FloatTensor(done).unsqueeze(1).to(device)
        )

    def __len__(self):
        # å›å‚³ç›®å‰ç·©è¡å€çš„å¤§å°
        return len(self.buffer)
        
# å®šç¾© Epsilon (æ¢ç´¢ç‡) éš¨æ­¥æ•¸è¡°æ¸›çš„å‡½æ•¸
def epsilon_by_step(step):
    # ä½¿ç”¨æŒ‡æ•¸è¡°æ¸›å…¬å¼è¨ˆç®—ç›®å‰çš„ Epsilon å€¼
    return EPSILON_END + (EPSILON_START - EPSILON_END) * np.exp(-1. * step / EPSILON_DECAY)

# è©•ä¼°å‡½å¼ï¼Œç”¨ä¾†æ¸¬è©¦ç›®å‰æ¨¡å‹çš„è¡¨ç¾ä¸¦å„²å­˜æˆ GIF
def evaluate(policy_net, env_name='CartPole-v1', filename='task1_eval.gif'):
    env = gym.make(env_name, render_mode='rgb_array')
    state, _ = env.reset()
    done = False
    total_reward = 0
    images = []

    while not done:
         # å°‡ç‹€æ…‹è½‰æ›æˆ Tensor
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
        with torch.no_grad():
             # æ ¹æ“š Q-value é¸æ“‡æœ€ä½³å‹•ä½œ (Greedy)
            action = policy_net(state_tensor).argmax().item()
        # åŸ·è¡Œå‹•ä½œä¸¦å–å¾—çµæœ
        state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        total_reward += reward
        images.append(env.render())

    imageio.mimsave(f"../vid_task1/{filename}", images, fps=30)
    print(f"ğŸ¥ Saved {filename} | Total Reward: {total_reward}")
    env.close()

# ==== è¨“ç·´å‡½æ•¸ ====
def train():
    env = gym.make("CartPole-v1")
    state_dim = env.observation_space.shape[0]  # ç‹€æ…‹ç©ºé–“çš„ç¶­åº¦
    action_dim = env.action_space.n # å‹•ä½œç©ºé–“çš„å¤§å°

    # åˆå§‹åŒ–ç­–ç•¥ç¶²è·¯ (Policy Network) å’Œç›®æ¨™ç¶²è·¯ (Target Network)
    policy_net = MLP(state_dim, action_dim).to(device)
    target_net = MLP(state_dim, action_dim).to(device)
    # å°‡ç­–ç•¥ç¶²è·¯çš„æ¬Šé‡è¤‡è£½çµ¦ç›®æ¨™ç¶²è·¯
    target_net.load_state_dict(policy_net.state_dict())
    # åˆå§‹åŒ–å„ªåŒ–å™¨ (ä½¿ç”¨ Adam)
    optimizer = optim.Adam(policy_net.parameters(), lr=LR)
    # åˆå§‹åŒ–ç¶“é©—å›æ”¾ç·©è¡å€
    memory = ReplayBuffer(MEMORY_SIZE)

    # å¦‚æœæœ‰æŒ‡å®š --resume åƒæ•¸ï¼Œå°±è¼‰å…¥å·²å„²å­˜çš„æ¨¡å‹æ¬Šé‡
    if args.resume:
        policy_net.load_state_dict(torch.load(args.resume))
        print(f"ğŸ“¦ Resumed from {args.resume}")

    step_count = 0
    best_reward = float('-inf')
    rewards, losses = [], []

    for episode in range(args.episodes):
        state, _ = env.reset()
        done = False
        total_reward = 0

        while not done:
            epsilon = epsilon_by_step(step_count)
            step_count += 1
            if random.random() < epsilon:
                action = env.action_space.sample()
            else:
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                action = policy_net(state_tensor).argmax().item()

            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            memory.push(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward

            if len(memory) > BATCH_SIZE:
                states, actions, rewards_b, next_states, dones = memory.sample(BATCH_SIZE)
                q_values = policy_net(states).gather(1, actions)
                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)
                expected_q = rewards_b + GAMMA * next_q_values * (1 - dones)
                loss = nn.MSELoss()(q_values, expected_q)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                losses.append(loss.item())

            if step_count % TARGET_UPDATE == 0:
                target_net.load_state_dict(policy_net.state_dict())

        rewards.append(total_reward)
        print(f"ğŸ® Episode {episode} - Reward: {total_reward:.2f} - Epsilon: {epsilon:.3f}")

        if episode % 50 == 0:
            torch.save(policy_net.state_dict(), f"../history_task1/task1_ep{episode}.pt")
            evaluate(policy_net, filename=f"task1_eval_ep{episode}.gif")

        if total_reward > best_reward:
            best_reward = total_reward
            torch.save(policy_net.state_dict(), f"../best_task1/task1_best.pt")
            evaluate(policy_net, filename=f"task1_best_ep{episode}.gif")

    torch.save(policy_net.state_dict(), f"../history_task1/task1_final.pt")
    env.close()

    # ==== ç•«åœ– ====
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(rewards, label='Reward')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.grid()
    plt.title('Episode Rewards')

    plt.subplot(1, 2, 2)
    plt.plot(losses, label='Loss', color='red')
    plt.xlabel('Training Step')
    plt.ylabel('Loss')
    plt.grid()
    plt.title('Loss Curve')

    plt.tight_layout()
    plt.savefig("../pic_task1/task1_result.png")
    plt.show()

if __name__ == "__main__":
    train()

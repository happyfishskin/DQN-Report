#task3
import gym
import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple
import matplotlib.pyplot as plt
import imageio
import os
from gym.wrappers import AtariPreprocessing, FrameStack

# ==== è¶…åƒæ•¸ ====
GAMMA = 0.99
LR = 1e-4
MEMORY_SIZE = 20000
BATCH_SIZE = 16
TARGET_UPDATE = 1000
EPSILON_START = 1.0
EPSILON_END = 0.01
EPSILON_DECAY = 1_000_000
EPISODES = 3000
MULTI_STEP = 3
ALPHA = 0.6                     # PER ä¸­ï¼Œæ±ºå®šå„ªå…ˆç´š p çš„é‡è¦æ€§ (p^alpha)
BETA_START = 0.4                # PER ä¸­ï¼Œé‡è¦æ€§æ¡æ¨£ (Importance Sampling) çš„åˆå§‹ beta å€¼
BETA_FRAMES = 1_000_000         # Beta å¾åˆå§‹å€¼å¢é•·åˆ° 1.0 æ‰€éœ€çš„æ­¥æ•¸

# ==== è‡ªå‹•å»ºç«‹è³‡æ–™å¤¾ ====
os.makedirs("../history_task32", exist_ok=True)
os.makedirs("../best_task32", exist_ok=True)
os.makedirs("../pic_task32", exist_ok=True)
os.makedirs("../vid_task32", exist_ok=True)


device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
print("CUDA available:", torch.cuda.is_available())
print("Using device:", device)

# ==== å»ºç«‹ Atari ç’°å¢ƒ ====
def make_env(env_name="ALE/Pong-v5", render_mode=None):
    base_env = gym.make(env_name, render_mode=render_mode, frameskip=1)
    env = AtariPreprocessing(base_env, grayscale_obs=True, scale_obs=True, frame_skip=4)
    env = FrameStack(env, num_stack=4)
    return env

# ==== Q-Network ====
class CNN(nn.Module):
    def __init__(self, action_dim):
        super(CNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(4, 32, 8, 4), nn.ReLU(),
            nn.Conv2d(32, 64, 4, 2), nn.ReLU(),
            nn.Conv2d(64, 64, 3, 1), nn.ReLU(),
        )
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 512), nn.ReLU(),
            nn.Linear(512, action_dim)
        )

    def forward(self, x):
        return self.fc(self.conv(x))

# ==== Multi-Step + Prioritized Experience Replay Buffer ====
Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))

# å¯¦ç¾äº† PER å’Œ Multi-Step Learning çš„ç¶“é©—å›æ”¾ç·©è¡å€
class PrioritizedReplayBuffer:
    def __init__(self, capacity, alpha, n_step):
        self.capacity = capacity
        self.buffer = []
        # ç”¨ä¸€å€‹ numpy array ä¾†å„²å­˜æ¯å€‹ç¶“é©—çš„å„ªå…ˆç´š (priority)
        self.priorities = np.zeros((capacity,), dtype=np.float32)
        self.pos = 0
        self.alpha = alpha
        self.n_step = n_step
        # æš«å­˜æœ€è¿‘ n_step æ­¥çš„ç¶“é©—ï¼Œç”¨æ–¼è¨ˆç®— n-step return
        self.nstep_buffer = deque(maxlen=n_step)

    def push(self, state, action, reward, next_state, done):
        # å°‡å–®æ­¥ç¶“é©—å­˜å…¥ n-step ç·©è¡å€
        transition = Transition(state, action, reward, next_state, done)
        self.nstep_buffer.append(transition)

        # å¦‚æœ n-step ç·©è¡å€é‚„æ²’æ»¿ï¼Œå°±ç›´æ¥è¿”å›
        if len(self.nstep_buffer) < self.n_step:
            return

        # è¨ˆç®— n-step return
        reward, next_state, done = self._get_n_step_info()
        # å–å‡º n-step ç·©è¡å€ä¸­æœ€æ—©çš„ state å’Œ action
        state, action = self.nstep_buffer[0].state, self.nstep_buffer[0].action

        # è¨­å®šæ–°ç¶“é©—çš„åˆå§‹å„ªå…ˆç´šç‚ºç•¶å‰æœ€é«˜å„ªå…ˆç´šï¼Œç¢ºä¿å®ƒæœ‰æ©Ÿæœƒè¢«æŠ½åˆ°
        max_prio = self.priorities.max() if self.buffer else 1.0
        # å°‡ n-step ç¶“é©—å­˜å…¥ä¸»ç·©è¡å€
        if len(self.buffer) < self.capacity:
            self.buffer.append((state, action, reward, next_state, done))
        else:
            self.buffer[self.pos] = (state, action, reward, next_state, done)
        # æ›´æ–°å°æ‡‰ä½ç½®çš„å„ªå…ˆç´š
        self.priorities[self.pos] = max_prio
        # æ›´æ–°å¯«å…¥ä½ç½®
        self.pos = (self.pos + 1) % self.capacity

    def _get_n_step_info(self):
        # è¨ˆç®— n-step return çš„æ ¸å¿ƒé‚è¼¯
        # å¾ n-step ç·©è¡å€çš„æœ€å¾Œä¸€ç­†ç¶“é©—é–‹å§‹å›æº¯
        reward, next_state, done = self.nstep_buffer[-1].reward, self.nstep_buffer[-1].next_state, self.nstep_buffer[-1].done
        # è¿´åœˆåå‘éæ­· n-step ç·©è¡å€ä¸­çš„å‰ n-1 ç­†ç¶“é©—
        for transition in reversed(list(self.nstep_buffer)[:-1]):
            r, n_s, d = transition.reward, transition.next_state, transition.done
            # ç´¯åŠ æŠ˜æ‰£å¾Œçš„çå‹µ
            reward = r + GAMMA * reward * (1 - d)
            # å¦‚æœä¸­é–“çš„æ­¥é©Ÿå°±çµæŸäº†ï¼Œé‚£éº¼æœ€çµ‚çš„ next_state å’Œ done å°±ç”¨é‚£ä¸€ç­†çš„
            next_state, done = (n_s, d) if d else (next_state, done)
        return reward, next_state, done

    def sample(self, batch_size, beta=0.4):
        if len(self.buffer) == 0:
            return [], [], [], [], [], [], []

        # æ ¹æ“šå„ªå…ˆç´šè¨ˆç®—æŠ½æ¨£æ¦‚ç‡
        prios = self.priorities[:len(self.buffer)]
        probs = prios ** self.alpha
        probs /= probs.sum()

        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        samples = [self.buffer[idx] for idx in indices]

        total = len(self.buffer)
        weights = (total * probs[indices]) ** (-beta)
        weights /= weights.max()
        weights = torch.FloatTensor(weights).unsqueeze(1).to(device)

        states, actions, rewards, next_states, dones = zip(*samples)

        return (
            torch.FloatTensor(np.array(states)).to(device),
            torch.LongTensor(actions).unsqueeze(1).to(device),
            torch.FloatTensor(rewards).unsqueeze(1).to(device),
            torch.FloatTensor(np.array(next_states)).to(device),
            torch.FloatTensor(dones).unsqueeze(1).to(device),
            weights, indices
        )

    def update_priorities(self, indices, priorities):
        for i, p in zip(indices, priorities):
            self.priorities[i] = p

    def __len__(self):
        return len(self.buffer)

# ==== æ¨¡æ“¬å½±ç‰‡ä¿å­˜ ====
def save_evaluation_gif(policy_net, env_name="ALE/Pong-v5", filename="pong_eval_final.gif"):
    env = make_env(env_name=env_name, render_mode="rgb_array")
    state, _ = env.reset()
    done = False
    images = []
    total_reward = 0

    while not done:
        with torch.no_grad():
            state_tensor = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0).to(device)
            q_values = policy_net(state_tensor)
            action = q_values.argmax().item()

        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        images.append(env.render())
        state = next_state
        total_reward += reward

    env.close()
    imageio.mimsave(f"../vid_task32/{filename}", images, fps=30)
    print(f"ğŸ¥ æ¨¡æ“¬å½±ç‰‡å·²å„²å­˜ç‚º ../vid_task32/{filename} | å¾—åˆ†: {total_reward}")

# ==== è¨“ç·´ä¸»å‡½æ•¸ ====
def train():
    env = make_env(render_mode=None)
    action_dim = env.action_space.n
    policy_net = CNN(action_dim).to(device)
    target_net = CNN(action_dim).to(device)
    target_net.load_state_dict(policy_net.state_dict())

    optimizer = optim.Adam(policy_net.parameters(), lr=LR)
    memory = PrioritizedReplayBuffer(MEMORY_SIZE, ALPHA, MULTI_STEP)

    step_count = 0
    episode_rewards = []
    losses = []
    best_reward = float('-inf')
    eval_counter = 0

    for episode in range(EPISODES):
        state, _ = env.reset()
        episode_reward = 0
        done = False

        while not done:
            epsilon = EPSILON_END + (EPSILON_START - EPSILON_END) * np.exp(-1. * step_count / EPSILON_DECAY)
            beta = min(1.0, BETA_START + step_count * (1.0 - BETA_START) / BETA_FRAMES)
            step_count += 1

            if random.random() < epsilon:
                action = env.action_space.sample()
            else:
                with torch.no_grad():
                    state_tensor = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0).to(device)
                    q_values = policy_net(state_tensor)
                    action = q_values.argmax().item()

            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            memory.push(state, action, reward, next_state, done)
            state = next_state
            episode_reward += reward

            if len(memory) > BATCH_SIZE:
                states, actions, rewards, next_states, dones, weights, indices = memory.sample(BATCH_SIZE, beta)
                with torch.no_grad():
                    next_actions = policy_net(next_states).argmax(1, keepdim=True)
                    next_q = target_net(next_states).gather(1, next_actions)
                    target_q = rewards + GAMMA**MULTI_STEP * next_q * (1 - dones)

                current_q = policy_net(states).gather(1, actions)
                loss = (current_q - target_q).pow(2) * weights
                prios = loss + 1e-5
                loss = loss.mean()

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                memory.update_priorities(indices, prios.data.cpu().numpy())
                losses.append(loss.item())

            if step_count % TARGET_UPDATE == 0:
                target_net.load_state_dict(policy_net.state_dict())

        episode_rewards.append(episode_reward)
        print(f"ğŸ® Episode {episode} - Reward: {episode_reward:.2f} - Epsilon: {epsilon:.3f}")

        if episode % 50 == 0:
            torch.save(policy_net.state_dict(), f"../history_task32/pong_ep{episode}.pt")
            print("ğŸ’¾ å¿«ç…§å·²å„²å­˜")

        # å„²å­˜æ–°æœ€ä½³æ¨¡å‹èˆ‡ GIF
        if episode_reward > best_reward:
            best_reward = episode_reward
            torch.save(policy_net.state_dict(), "../best_task32/best_model.pt")
            print(f"ğŸ† æ–°æœ€ä½³æ¨¡å‹ï¼ˆReward: {best_reward:.2f}ï¼‰ï¼é€²è¡Œè©•ä¼°...")

            # å¼·åˆ¶åŠ è¼‰æœ€æ–°æ¨¡å‹
            policy_net.load_state_dict(torch.load("../best_task32/best_model.pt"))
            
            # å„²å­˜ GIF
            save_evaluation_gif(policy_net, filename=f"pong_eval_best_ep{episode}.gif")
        else:
            print(f"Episode {episode} did not surpass best reward.")

    torch.save(policy_net.state_dict(), "../history_task32/LAB5_613k0007c_task3_pong_final.pt")
    env.close()

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(episode_rewards, label='Episode Reward')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title('Reward Curve')
    plt.grid()

    plt.subplot(1, 2, 2)
    plt.plot(losses, label='Loss', color='red')
    plt.xlabel('Training Step')
    plt.ylabel('Loss')
    plt.title('Loss Curve')
    plt.grid()

    plt.tight_layout()
    plt.savefig("../pic_task32/LAB5_613k0007c_task3_reward_loss.png")
    plt.show()

    save_evaluation_gif(policy_net)  # æœ€çµ‚è©•ä¼°
    return policy_net

if __name__ == "__main__":
    model = train()
